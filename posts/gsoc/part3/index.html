<!DOCTYPE html>
<html lang="en">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <title>
  Google Summer of Code - Part 3 · Sai&#39;s Blog
</title>
  <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="color-scheme" content="light dark">




<meta name="author" content="Sai Wong">
<meta name="description" content="Continuing my GSoC project: building SBOL-to-ML functionality">
<meta name="keywords" content="blog, personal, hugo">



  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Google Summer of Code - Part 3">
  <meta name="twitter:description" content="Continuing my GSoC project: building SBOL-to-ML functionality">

<meta property="og:url" content="http://localhost:1313/posts/gsoc/part3/">
  <meta property="og:site_name" content="Sai&#39;s Blog">
  <meta property="og:title" content="Google Summer of Code - Part 3">
  <meta property="og:description" content="Continuing my GSoC project: building SBOL-to-ML functionality">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-09-26T15:18:35-07:00">
    <meta property="article:modified_time" content="2025-09-26T15:18:35-07:00">
    <meta property="article:tag" content="Gsoc">
    <meta property="article:tag" content="Blog">




<link rel="canonical" href="http://localhost:1313/posts/gsoc/part3/">


<link rel="preload" href="/fonts/fa-brands-400.woff2" as="font" type="font/woff2" crossorigin>
<link rel="preload" href="/fonts/fa-regular-400.woff2" as="font" type="font/woff2" crossorigin>
<link rel="preload" href="/fonts/fa-solid-900.woff2" as="font" type="font/woff2" crossorigin>


  
  
  <link rel="stylesheet" href="/css/coder.css" media="screen">








 




<link rel="icon" type="image/svg+xml" href="/images/favicon.svg" sizes="any">
<link rel="icon" type="image/png" href="/images/favicon-32x32.png" sizes="32x32">
<link rel="icon" type="image/png" href="/images/favicon-16x16.png" sizes="16x16">

<link rel="apple-touch-icon" href="/images/apple-touch-icon.png">
<link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">

<link rel="manifest" href="/site.webmanifest">
<link rel="mask-icon" href="/images/safari-pinned-tab.svg" color="#5bbad5">









</head>




<body class="preload-transitions colorscheme-light">
  
<div class="float-container">
    <a id="dark-mode-toggle" class="colorscheme-toggle">
        <i class="fa-solid fa-adjust fa-fw" aria-hidden="true"></i>
    </a>
</div>


  <main class="wrapper">
    <nav class="navigation">
  <section class="container">
    
    <a class="navigation-title" href="http://localhost:1313/">
      Sai&#39;s Blog
    </a>
    
    
      <input type="checkbox" id="menu-toggle" />
      <label class="menu-button float-right" for="menu-toggle">
        <i class="fa-solid fa-bars fa-fw" aria-hidden="true"></i>
      </label>
      <ul class="navigation-list">
        
          
            <li class="navigation-item">
              <a class="navigation-link " href="/posts/">Posts</a>
            </li>
          
        
        
      </ul>
    
  </section>
</nav>


    <div class="content">
      
  <section class="container post">
    <article>
      <header>
        <div class="post-title">
          <h1 class="title">
            <a class="title-link" href="http://localhost:1313/posts/gsoc/part3/">
              Google Summer of Code - Part 3
            </a>
          </h1>
        </div>
        <div class="post-meta">
          <div class="date">
            <span class="posted-on">
              <i class="fa-solid fa-calendar" aria-hidden="true"></i>
              <time datetime="2025-09-26T15:18:35-07:00">
                2025-09-26
              </time>
            </span>
            <span class="reading-time">
              <i class="fa-solid fa-clock" aria-hidden="true"></i>
              7-minute read
            </span>
          </div>
          
          <div class="categories">
  <i class="fa-solid fa-folder" aria-hidden="true"></i>
    <a href="/categories/projects/">Projects</a></div>

          <div class="tags">
  <i class="fa-solid fa-tag" aria-hidden="true"></i>
    <span class="tag">
      <a href="/tags/gsoc/">Gsoc</a>
    </span>
      <span class="separator">•</span>
    <span class="tag">
      <a href="/tags/blog/">Blog</a>
    </span></div>

        </div>
      </header>

      <div class="post-content">
        
        <p><a href="http://localhost:1313/posts/gsoc/gsoc/" >Read part one here!</a></p>
<p><a href="http://localhost:1313/posts/gsoc/part2/" >Read part two here!</a></p>
<p><!-- raw HTML omitted --><a href="https://github.com/SynBioDex/SeqTrainer"  class="external-link" target="_blank" rel="noopener">GitHub Repository</a> All work before and including commit 3761fd7 was done by me during GSoC.<!-- raw HTML omitted --></p>
<h2 id="task-3">
  Task 3
  <a class="heading-link" href="#task-3">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h2>
<p>With the core package features finished, we were able to replicate the results of the <!-- raw HTML omitted --><a href="https://elifesciences.org/reviewed-preprints/92558"  class="external-link" target="_blank" rel="noopener">research paper</a><!-- raw HTML omitted --> with similar metrics. However, because those metrics were low and did not show high predictive power, we wanted to try out graph neural networks for this purpose. We believed that graph neural networks would be perfect for our data because SBOL is inherently a graph. Typical synthetic biology machine learning models use only the DNA sequence to make predictions; however, this approach lacks the experimental metadata that can be imperative for prediction. Because SBOL captures this metadata, encoding this information in a graph neural network would provide more context to the model and theoretically allow for higher accuracies.</p>
<hr>
<h2 id="autordf2gml-for-graph-conversion">
  AutoRDF2GML for Graph Conversion
  <a class="heading-link" href="#autordf2gml-for-graph-conversion">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h2>
<p>To begin, we used a tool called AutoRDF2GML, designed for converting an RDF file to a format suitable for graph neural network training. The main thing we had to resolve to use this tool was the config file; we needed to input our nodes, relationships, and models, which had to be customized to the data. If we wanted this workflow to be reproducible for other forms of data, this config would need to be auto-generated.</p>
<p>Later on, I wrote a script that automates generation of this config by using SPARQL queries to:</p>
<ul>
<li>Fetch major nodes we cared about (ComponentDefinition, ModuleDefinition, SequenceAnnotation)</li>
<li>Fetch the URIs of connecting relationships needed for the config file</li>
</ul>
<p>Once the config was parsed, the tool ran and generated several CSV files containing the edge mappings and node features.</p>
<hr>
<h2 id="building-graph-data-for-pytorch-geometric">
  Building Graph Data for PyTorch Geometric
  <a class="heading-link" href="#building-graph-data-for-pytorch-geometric">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h2>
<p>I wrote pandas scripts that took those mappings and features to create a PyTorch Geometric <code>HeteroData</code> object, which is used as training data for graph neural networks in PyTorch. Once I had created these scripts, I essentially had a pipeline that went from SBOL files on my computer, processed through the AutoRDF2GML tool, and turned into multiple <code>HeteroData</code> objects.</p>
<p>To start, I created a straightforward GNN through PyTorch Geometric, which took in heterogeneous graphs and applied different convolutions for several stages of message passing. Our example was fitted towards a graph-level regression problem, as we wanted to predict the RNA expression level of the entire SBOL graph. Once I configured the PyTorch setup and processed all 17K files through AutoRDF2GML (which took days of running), the GNN was ready to be trained.</p>
<hr>
<h2 id="training-challenges">
  Training Challenges
  <a class="heading-link" href="#training-challenges">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h2>
<p>After running the GNN for several epochs, we noticed that the validation and training loss was very sporadic, jumping from very high values to very low values. I discovered that this was because our target label, the RNA expression level, was very skewed and had outliers much larger than the rest of the distribution. To account for this, I applied a log transformation and retrained the model; seeing lower and more consistent values for the training and validation loss indicated an improvement.</p>
<p>However, I noticed that the training and validation loss then remained relatively constant throughout each epoch, meaning the model was barely learning anything from the data. After further experimentation and reflection, I realized that one important reason the model wasn’t learning lay inherently in the data. Our data, because it belonged to a single experiment where only the DNA sequence would change, was encoded with essentially the same graph structure with only a differing DNA sequence. There was not enough variation in our data, which means the model would not be able to generalize or learn anything new with such little variation.</p>
<hr>
<h2 id="reflections">
  Reflections
  <a class="heading-link" href="#reflections">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h2>
<p>Moreover, I realized that in converting the SBOL files using AutoRDF2GML, the tool would use SciBERT to vectorize some of the RDF features; however, since our features included the DNA sequence (which was the main, changing component of our graphs), SciBERT would encode the DNA sequence as a “scientific term,” even though a DNA sequence would just be a jumble of random characters to the model.</p>
<p>We concluded that the GNN was still a very useful application, but we needed to work on:</p>
<ul>
<li>Getting a better DNA encoding</li>
<li>Using more varied data</li>
</ul>
<hr>
<h2 id="task-4">
  Task 4
  <a class="heading-link" href="#task-4">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h2>
<p>In reflection to the previous task, we wanted to test other models that would be good for encoding DNA properties and generating strong results. We decided to try DNABERT, a transformer-based model, because it would be the most complex, was trained on a very large dataset of DNA, and would presumably generate a meaningful encoding for our DNA.</p>
<hr>
<h2 id="dnabert-setup-and-fine-tuning">
  DNABERT Setup and Fine-Tuning
  <a class="heading-link" href="#dnabert-setup-and-fine-tuning">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h2>
<p>To start, I began getting familiar with the Hugging Face Transformers library in Python for fine-tuning a transformer. I researched how fine-tuning is normally done and looked at several code snippets for fine-tuning DNABERT. Adapting this to my dataset, I wanted to fine-tune DNABERT for a regression task, so I added a regression head on top of the DNABERT layer. Moreover, I learned about LoRA fine-tuning, which utilizes low-rank adaptation to decrease the number of weights to fine-tune so that it wouldn’t be too computationally intensive.</p>
<p>In creating this workflow, I had a notebook that took our existing 17K rows of data, tokenized them using the DNABERT tokenizer, split into training, test, and evaluation datasets, then utilized the <code>Trainer</code> class in Hugging Face to fine-tune the model with some initial hyperparameters and using LoRA. Eventually, the model compiled and was able to be fine-tuned, but since there wasn’t much data or hyperparameter optimization, and I couldn’t run this fine-tuning on my local computer, our results/evaluation of this method using DNABERT could not be finalized.</p>
<hr>
<h2 id="hpc-experiments-and-hyperparameter-tuning">
  HPC Experiments and Hyperparameter Tuning
  <a class="heading-link" href="#hpc-experiments-and-hyperparameter-tuning">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h2>
<p>Later we were able to get access to an HPC, so the problems I had with running these training/preprocessing methods on my local computer would be bypassed. To prepare my data to be run on the HPC, I created several scripts for hyperparameter tuning. We also decided to switch to a similar dataset from the same research paper that had a total of 600K entries because of this new access to the HPC.</p>
<p>I created:</p>
<ul>
<li>One Python script for hyperparameter tuning a scikit-learn model (RandomForest, LogisticRegression) to test our preprocessing package and aim for better results</li>
<li>Another notebook that took the transformer fine-tuning and added an additional hyperparameter tuning section using Optuna to find parameters</li>
</ul>
<hr>
<h2 id="conclusion">
  Conclusion
  <a class="heading-link" href="#conclusion">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h2>
<p>This was the last task I completed, so we were not able to test and get results for this, which would be the next step. In the end, I learned a lot about SBOL, creating novel neural networks, working with transformers, and applying preprocessing methods through SPARQL queries and interacting with triple-store databases. We ended up with a Python package that is published on PyPI for researchers to use. Though we wanted our Python package to include a module that could create a pipeline for graph neural network training, we established a working pipeline going from SBOL files to standard machine learning training on DNA features. We also established a good framework for exploring the potential of graph neural networks for enhanced prediction by incorporating experimental metadata, with some components of this pipeline already automated. Furthermore, we set up a strong workflow for fine-tuning a transformer to determine if this may improve prediction quality for DNA features on our dataset, which will also be useful for completing the graph neural network training pipeline.</p>
<p>Thank you to Gonzalo Vidal and Chris Myers for being my mentors in this project, and to the National Resource for Network Biology for accepting this project under the Google Summer of Code program!</p>

      </div>


      <footer>
        


        
        
        
        
        
        
        
      </footer>
    </article>

    
  </section>

    </div>

    <footer class="footer">
  <section class="container">
    ©
    
    2025
     Sai Wong 
    ·
    
    Powered by <a href="https://gohugo.io/" target="_blank" rel="noopener">Hugo</a> & <a href="https://github.com/luizdepra/hugo-coder/" target="_blank" rel="noopener">Coder</a>.
    
  </section>
</footer>

  </main>

  

  
  
  <script src="/js/coder.js"></script>
  

  

  


  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  
</body>
</html>
